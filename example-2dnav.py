import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw
import random
from typing import List, Tuple, Optional
import heapq
import os
import argparse
import time
from pathlib import Path

# HuggingFace Diffusers imports
from diffusers import DDPMScheduler, UNet2DConditionModel
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# ============================================================================
# 1. Enhanced Scene Generation with Better Obstacle Patterns
# ============================================================================

class AdvancedObstacleGenerator:
    """ë‹¤ì–‘í•œ ë³µì¡í•œ ì¥ì• ë¬¼ íŒ¨í„´ì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, image_size=(64, 64)):
        self.image_size = image_size
        
    def generate_maze_pattern(self):
        """ë¯¸ë¡œ í˜•íƒœì˜ ì¥ì• ë¬¼ íŒ¨í„´ ìƒì„±"""
        obstacle_map = np.zeros(self.image_size, dtype=np.uint8)
        
        wall_thickness = 3
        corridor_width = 8
        
        # ìˆ˜ì§ ë²½ ìƒì„±
        for x in range(corridor_width, self.image_size[0], corridor_width + wall_thickness):
            obstacle_map[:, x:x+wall_thickness] = 1
            
        # ìˆ˜í‰ ë²½ ìƒì„± (êµ¬ë© í¬í•¨)
        for y in range(corridor_width, self.image_size[1], corridor_width + wall_thickness):
            wall_row = obstacle_map[y:y+wall_thickness, :]
            wall_row[:] = 1
            # ë²½ì— êµ¬ë© ìƒì„±
            gap_positions = np.random.choice(self.image_size[0]//2, 2, replace=False) * 2
            for gap in gap_positions:
                gap_start = max(0, gap - 2)
                gap_end = min(self.image_size[0], gap + 3)
                wall_row[:, gap_start:gap_end] = 0
        
        return obstacle_map
    
    def generate_scattered_obstacles(self):
        """ì‚°ì¬ëœ ì›í˜• ë° ì‚¬ê°í˜• ì¥ì• ë¬¼ ìƒì„±"""
        obstacle_map = np.zeros(self.image_size, dtype=np.uint8)
        
        num_obstacles = np.random.randint(8, 15)
        
        for _ in range(num_obstacles):
            if np.random.random() < 0.6:  # ì›í˜• ì¥ì• ë¬¼
                center_x = np.random.randint(5, self.image_size[0] - 5)
                center_y = np.random.randint(5, self.image_size[1] - 5)
                radius = np.random.randint(3, 8)
                
                y, x = np.ogrid[:self.image_size[0], :self.image_size[1]]
                mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2
                obstacle_map[mask] = 1
                
            else:  # ì‚¬ê°í˜• ì¥ì• ë¬¼
                x = np.random.randint(2, self.image_size[0] - 8)
                y = np.random.randint(2, self.image_size[1] - 8)
                w = np.random.randint(3, 6)
                h = np.random.randint(3, 6)
                obstacle_map[y:y+h, x:x+w] = 1
        
        return obstacle_map
    
    def generate_corridor_pattern(self):
        """ì¢ì€ ë³µë„ íŒ¨í„´ ìƒì„±"""
        obstacle_map = np.ones(self.image_size, dtype=np.uint8)
        
        # ë©”ì¸ ë³µë„ ìƒì„±
        corridor_width = np.random.randint(4, 8)
        corridor_y = self.image_size[1] // 2
        corridor_start = corridor_y - corridor_width // 2
        corridor_end = corridor_y + corridor_width // 2
        
        obstacle_map[corridor_start:corridor_end, :] = 0
        
        # ë¶„ê¸° ë³µë„ ìƒì„±
        num_branches = np.random.randint(2, 4)
        for _ in range(num_branches):
            branch_x = np.random.randint(10, self.image_size[0] - 10)
            branch_length = np.random.randint(8, 15)
            branch_width = np.random.randint(3, 5)
            
            if np.random.random() < 0.5:  # ìˆ˜ì§ ë¶„ê¸°
                branch_start = max(0, corridor_y - branch_length // 2)
                branch_end = min(self.image_size[1], corridor_y + branch_length // 2)
                obstacle_map[branch_start:branch_end, 
                           branch_x:branch_x+branch_width] = 0
            else:  # ìˆ˜í‰ í™•ì¥
                obstacle_map[corridor_start:corridor_end, 
                           branch_x:branch_x+branch_length] = 0
        
        return obstacle_map

def generate_challenging_navigation_scene(pattern_type=None):
    """ë„ì „ì ì¸ ë„¤ë¹„ê²Œì´ì…˜ ì¥ë©´ ìƒì„±"""
    generator = AdvancedObstacleGenerator()
    
    if pattern_type is None:
        pattern_type = np.random.choice(['maze', 'scattered', 'corridor'])
    
    if pattern_type == 'maze':
        obstacle_map = generator.generate_maze_pattern()
    elif pattern_type == 'scattered':
        obstacle_map = generator.generate_scattered_obstacles()
    else:
        obstacle_map = generator.generate_corridor_pattern()
    
    # RGB ì‹œê°í™” ìƒì„±
    rgb_image = np.ones((64, 64, 3), dtype=np.uint8) * 255  # í°ìƒ‰ ë°°ê²½
    rgb_image[obstacle_map == 1] = [50, 50, 50]  # ì–´ë‘ìš´ ì¥ì• ë¬¼
    
    return rgb_image, obstacle_map, pattern_type

# ============================================================================
# 2. A* Pathfinder for Ground Truth Generation
# ============================================================================

class AStarPathfinder:
    """ìµœì  ê²½ë¡œ ìƒì„±ì„ ìœ„í•œ A* ê²½ë¡œ ì°¾ê¸° ì•Œê³ ë¦¬ì¦˜"""
    
    def __init__(self, obstacle_map):
        self.obstacle_map = obstacle_map
        self.height, self.width = obstacle_map.shape
        
    def heuristic(self, a, b):
        return np.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)
    
    def get_neighbors(self, pos):
        neighbors = []
        directions = [(-1,-1, 1.414), (-1,0, 1.0), (-1,1, 1.414), 
                     (0,-1, 1.0), (0,1, 1.0), 
                     (1,-1, 1.414), (1,0, 1.0), (1,1, 1.414)]
        
        for dx, dy, cost in directions:
            new_x, new_y = pos[0] + dx, pos[1] + dy
            
            if (0 <= new_x < self.height and 0 <= new_y < self.width and 
                self.obstacle_map[new_x, new_y] == 0):
                neighbors.append(((new_x, new_y), cost))
        
        return neighbors
    
    def find_path(self, start, goal):
        start, goal = tuple(start), tuple(goal)
        
        open_set = [(0, start)]
        came_from = {}
        g_score = {start: 0}
        f_score = {start: self.heuristic(start, goal)}
        
        while open_set:
            current = heapq.heappop(open_set)[1]
            
            if current == goal:
                path = []
                while current in came_from:
                    path.append(current)
                    current = came_from[current]
                path.append(start)
                return path[::-1]
            
            for neighbor, cost in self.get_neighbors(current):
                tentative_g = g_score[current] + cost
                
                if neighbor not in g_score or tentative_g < g_score[neighbor]:
                    came_from[neighbor] = current
                    g_score[neighbor] = tentative_g
                    f_score[neighbor] = tentative_g + self.heuristic(neighbor, goal)
                    heapq.heappush(open_set, (f_score[neighbor], neighbor))
        
        return []

# ============================================================================
# 3. Collision Detection Functions (FIXED)
# ============================================================================

def sample_obstacle_map_soft_gradients(point, obstacle_map, map_size):
    """ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ë³´ì¡´í•˜ëŠ” ì¥ì• ë¬¼ ë§µ ìƒ˜í”Œë§ (ìˆ˜ì •ë¨)"""
    # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ ë§µ ì¢Œí‘œë¡œ ë³€í™˜
    x_continuous = point[0] * (map_size - 1)
    y_continuous = point[1] * (map_size - 1)
    
    # ìœ íš¨ ë²”ìœ„ë¡œ í´ë¨í”„ (ë¯¸ë¶„ ê°€ëŠ¥)
    x_continuous = torch.clamp(x_continuous, 0, map_size - 1)
    y_continuous = torch.clamp(y_continuous, 0, map_size - 1)
    
    # ê·¸ë¦¬ë“œ ìƒ˜í”Œë§ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ [-1, 1]
    grid_x = (x_continuous / (map_size - 1)) * 2 - 1
    grid_y = (y_continuous / (map_size - 1)) * 2 - 1
    
    # ë‹¨ì¼ í¬ì¸íŠ¸ë¥¼ ìœ„í•œ ìƒ˜í”Œë§ ê·¸ë¦¬ë“œ [1, 1, 1, 2] ìƒì„±
    grid = torch.stack([grid_x, grid_y], dim=-1).unsqueeze(0).unsqueeze(0)
    
    # ì¥ì• ë¬¼ ë§µì— ë°°ì¹˜ ë° ì±„ë„ ì°¨ì› ì¶”ê°€ [1, 1, H, W]
    obstacle_map_batch = obstacle_map.unsqueeze(0).unsqueeze(0)
    
    # ë¯¸ë¶„ ê°€ëŠ¥í•œ ìƒ˜í”Œë§ì„ ìœ„í•´ F.grid_sample ì‚¬ìš©
    try:
        sampled_value = F.grid_sample(
            obstacle_map_batch, grid, 
            mode='bilinear', padding_mode='border', align_corners=True
        )
        return sampled_value.squeeze()
    except Exception:
        # í´ë°±: ê°„ë‹¨í•œ ì´ì¤‘ì„ í˜• ë³´ê°„
        return sample_obstacle_map_bilinear_fallback(point, obstacle_map, map_size)

def sample_obstacle_map_bilinear_fallback(point, obstacle_map, map_size):
    """ê·¸ë¦¬ë“œ ìƒ˜í”Œë§ ì‹¤íŒ¨ ì‹œ í´ë°± í•¨ìˆ˜"""
    x_continuous = point[0] * (map_size - 1)
    y_continuous = point[1] * (map_size - 1)
    
    x_continuous = torch.clamp(x_continuous, 0, map_size - 1)
    y_continuous = torch.clamp(y_continuous, 0, map_size - 1)
    
    x_floor = torch.floor(x_continuous).long()
    y_floor = torch.floor(y_continuous).long()
    x_ceil = torch.clamp(x_floor + 1, 0, map_size - 1)
    y_ceil = torch.clamp(y_floor + 1, 0, map_size - 1)
    
    x_frac = x_continuous - x_floor.float()
    y_frac = y_continuous - y_floor.float()
    
    try:
        top_left = obstacle_map[y_floor, x_floor]
        top_right = obstacle_map[y_floor, x_ceil]
        bottom_left = obstacle_map[y_ceil, x_floor]
        bottom_right = obstacle_map[y_ceil, x_ceil]
        
        top = top_left * (1 - x_frac) + top_right * x_frac
        bottom = bottom_left * (1 - x_frac) + bottom_right * x_frac
        collision_value = top * (1 - y_frac) + bottom * y_frac
        
        return collision_value
    except:
        return torch.tensor(0.0, device=point.device, requires_grad=True)

def compute_noise_based_collision_penalty(noise_pred, noisy_paths, timesteps, scheduler, obstacle_maps, device):
    """ë…¸ì´ì¦ˆ ì˜ˆì¸¡ì—ì„œ ì§ì ‘ ì¶©ëŒ í˜ë„í‹° ê³„ì‚° (ìˆ˜ì •ë¨)"""
    batch_size = noise_pred.shape[0]
    map_size = obstacle_maps.shape[-1]
    
    # íƒ€ì„ìŠ¤í… ì²˜ë¦¬ ìˆ˜ì •
    if isinstance(timesteps, torch.Tensor):
        if len(timesteps.shape) == 0:  # ìŠ¤ì¹¼ë¼ í…ì„œ
            timesteps = timesteps.unsqueeze(0).expand(batch_size)
        elif len(timesteps) == 1:  # ì „ì²´ ë°°ì¹˜ì— ëŒ€í•œ ë‹¨ì¼ íƒ€ì„ìŠ¤í…
            timesteps = timesteps.expand(batch_size)
    else:
        timesteps = torch.tensor([timesteps] * batch_size, device=device)
    
    # ê° ë°°ì¹˜ ìƒ˜í”Œì— ëŒ€í•œ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ ê°’ ê°€ì ¸ì˜¤ê¸°
    alpha_t = scheduler.alphas_cumprod[timesteps].to(device)
    sqrt_alpha_t = torch.sqrt(alpha_t).view(-1, 1, 1)
    sqrt_one_minus_alpha_t = torch.sqrt(1 - alpha_t).view(-1, 1, 1)
    
    # DDPM ë””ë…¸ì´ì§• ê³µì‹: x_0 â‰ˆ (x_t - sqrt(1-Î±_t) * Îµ_Î¸) / sqrt(Î±_t)
    predicted_original = (noisy_paths - sqrt_one_minus_alpha_t * noise_pred) / sqrt_alpha_t
    
    total_penalty = torch.tensor(0.0, device=device, requires_grad=True)
    
    for b in range(batch_size):
        path = predicted_original[b]
        obstacle_map = obstacle_maps[b].float()
        
        batch_penalty = torch.tensor(0.0, device=device, requires_grad=True)
        
        # í¬ì¸íŠ¸ ì¶©ëŒ (ê·¸ë˜ë””ì–¸íŠ¸ í¬í•¨!)
        for i in range(path.shape[0]):
            point_collision = sample_obstacle_map_soft_gradients(path[i], obstacle_map, map_size)
            batch_penalty = batch_penalty + point_collision
        
        # ì„ ë¶„ ì¶©ëŒ (ê·¸ë˜ë””ì–¸íŠ¸ í¬í•¨!)
        for i in range(path.shape[0] - 1):
            start_point = path[i]
            end_point = path[i + 1]
            
            # ê° ì„ ë¶„ì„ ë”°ë¼ 3ê°œ í¬ì¸íŠ¸ ìƒ˜í”Œë§
            for j in range(1, 4):
                t = j / 4
                interpolated_point = start_point * (1 - t) + end_point * t
                line_collision = sample_obstacle_map_soft_gradients(interpolated_point, obstacle_map, map_size)
                batch_penalty = batch_penalty + 0.7 * line_collision
        
        total_penalty = total_penalty + batch_penalty / (path.shape[0] + 3 * (path.shape[0] - 1) * 0.7)
    
    return total_penalty / batch_size

# ============================================================================
# 4. Diffusion Path Planning Model
# ============================================================================

class PathDiffusionModel(nn.Module):
    """HuggingFace Diffusersë¥¼ ì‚¬ìš©í•œ ê²½ë¡œ ê³„íšìš© í™•ì‚° ëª¨ë¸"""
    
    def __init__(self, max_path_length=16, path_dim=2):
        super(PathDiffusionModel, self).__init__()
        
        self.max_path_length = max_path_length
        self.path_dim = path_dim
        
        # í™˜ê²½ ì¸ì½”ë” (ì¥ì• ë¬¼ ë§µ + ì‹œì‘/ëª©í‘œ ì²˜ë¦¬)
        self.env_encoder = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 64 -> 32
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),  # 32 -> 16
            nn.AdaptiveAvgPool2d(8),  # -> 8x8
            nn.Flatten(),
            nn.Linear(128 * 8 * 8, 512),
            nn.ReLU(),
            nn.Linear(512, 256)
        )
        
        # ì‹œì‘/ëª©í‘œ ìœ„ì¹˜ ì¸ì½”ë”
        self.pos_encoder = nn.Sequential(
            nn.Linear(4, 64),  # [start_x, start_y, goal_x, goal_y]
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 64)
        )
        
        # í™•ì‚°ì„ ìœ„í•œ ì‹œê°„ ì„ë² ë”©
        self.time_embedding = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 128)
        )
        
        # UNetì„ ìœ„í•œ ê²½ë¡œ íˆ¬ì˜
        self.path_projection = nn.Linear(max_path_length * path_dim, 16 * 16)
        self.path_unprojection = nn.Linear(16 * 16, max_path_length * path_dim)
        
        # UNet ì´ˆê¸°í™” (ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨)
        try:
            self.unet = UNet2DConditionModel(
                sample_size=16,
                in_channels=1,
                out_channels=1,
                layers_per_block=2,
                block_out_channels=(64, 128, 256),
                down_block_types=(
                    "DownBlock2D",
                    "CrossAttnDownBlock2D", 
                    "CrossAttnDownBlock2D"
                ),
                up_block_types=(
                    "CrossAttnUpBlock2D",
                    "CrossAttnUpBlock2D",
                    "UpBlock2D"
                ),
                cross_attention_dim=256 + 64,
                attention_head_dim=32
            )
            self.use_conditioning = True
            print("ì¡°ê±´ë¶€ UNet2DConditionModel ì‚¬ìš©")
        except Exception as e:
            print(f"ì¡°ê±´ë¶€ UNet ì‹¤íŒ¨: {e}")
            try:
                self.unet = UNet2DConditionModel(
                    sample_size=16,
                    in_channels=1,
                    out_channels=1,
                    block_out_channels=(64, 128, 256),
                    layers_per_block=2,
                    cross_attention_dim=320
                )
                self.use_conditioning = True
                print("ë‹¨ìˆœ UNet2DConditionModel ì‚¬ìš©")
            except Exception as e2:
                print(f"ë‹¨ìˆœ UNetë„ ì‹¤íŒ¨: {e2}")
                from diffusers import UNet2DModel
                self.unet = UNet2DModel(
                    sample_size=16,
                    in_channels=1,
                    out_channels=1,
                    block_out_channels=(64, 128, 256),
                    layers_per_block=2
                )
                self.use_conditioning = False
                print("ê¸°ë³¸ UNet2DModel ì‚¬ìš© (ì¡°ê±´ë¶€ ì—†ìŒ)")
        
        # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = DDPMScheduler(
            num_train_timesteps=1000,
            beta_start=0.0001,
            beta_end=0.02,
            beta_schedule="linear",
            prediction_type="epsilon"
        )
    
    def get_time_embedding(self, timesteps):
        """ì •í˜„íŒŒ ì‹œê°„ ì„ë² ë”© ìƒì„±"""
        half_dim = 128
        emb = np.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)
        emb = timesteps[:, None] * emb[None, :]
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
        
        if emb.shape[1] < 256:
            emb = F.pad(emb, (0, 256 - emb.shape[1]))
        elif emb.shape[1] > 256:
            emb = emb[:, :256]
            
        return self.time_embedding(emb)
    
    def encode_conditions(self, obstacle_map, start_pos, goal_pos):
        """í™˜ê²½ ì¡°ê±´ ì¸ì½”ë”©"""
        env_features = self.env_encoder(obstacle_map.unsqueeze(1))
        pos_input = torch.cat([start_pos, goal_pos], dim=-1)
        pos_features = self.pos_encoder(pos_input)
        condition_embedding = torch.cat([env_features, pos_features], dim=-1)
        return condition_embedding
    
    def forward(self, noisy_paths, timesteps, obstacle_map, start_pos, goal_pos):
        """í›ˆë ¨ìš© ìˆœì „íŒŒ"""
        batch_size = noisy_paths.shape[0]
        
        # ì‹œê°„ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
        time_emb = self.get_time_embedding(timesteps)
        
        # ì¡°ê±´ ì¸ì½”ë”©
        condition_emb = self.encode_conditions(obstacle_map, start_pos, goal_pos)
        
        # UNetì„ ìœ„í•´ ê²½ë¡œë¥¼ ì´ë¯¸ì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        path_flat = noisy_paths.view(batch_size, -1)
        path_img = self.path_projection(path_flat).view(batch_size, 1, 16, 16)
        
        # ì ì ˆí•œ ì¡°ê±´ë¶€ ì²˜ë¦¬ë¡œ UNet ë””ë…¸ì´ì§•
        if self.use_conditioning:
            try:
                noise_pred = self.unet(
                    sample=path_img,
                    timestep=timesteps,
                    encoder_hidden_states=condition_emb.unsqueeze(1),
                    return_dict=False
                )[0]
            except Exception:
                try:
                    noise_pred = self.unet(
                        path_img,
                        timesteps,
                        condition_emb.unsqueeze(1),
                        return_dict=False
                    )[0]
                except Exception:
                    noise_pred = self.unet(path_img, timesteps, return_dict=False)[0]
        else:
            noise_pred = self.unet(path_img, timesteps, return_dict=False)[0]
        
        # ê²½ë¡œ í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ë³€í™˜
        noise_pred_flat = noise_pred.view(batch_size, -1)
        noise_pred_path = self.path_unprojection(noise_pred_flat)
        noise_pred_path = noise_pred_path.view(batch_size, self.max_path_length, self.path_dim)
        
        return noise_pred_path
    
    @torch.no_grad()
    def generate_path(self, obstacle_map, start_pos, goal_pos, num_inference_steps=50):
        """í™•ì‚° ìƒ˜í”Œë§ì„ ì‚¬ìš©í•œ ê²½ë¡œ ìƒì„±"""
        batch_size = obstacle_map.shape[0]
        device = obstacle_map.device
        
        # ëœë¤ ë…¸ì´ì¦ˆë¡œ ì‹œì‘
        path_shape = (batch_size, self.max_path_length, self.path_dim)
        path = torch.randn(path_shape, device=device)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        self.scheduler.set_timesteps(num_inference_steps)
        
        # ì¡°ê±´ ì¸ì½”ë”© (ì¡°ê±´ë¶€ ì‚¬ìš© ì‹œ)
        if self.use_conditioning:
            condition_emb = self.encode_conditions(obstacle_map, start_pos, goal_pos)
        
        # ë””ë…¸ì´ì§• ë£¨í”„
        for timestep in self.scheduler.timesteps:
            timesteps = timestep.expand(batch_size).to(device)
            
            # UNetì„ ìœ„í•´ ë³€í™˜
            path_flat = path.view(batch_size, -1)
            path_img = self.path_projection(path_flat).view(batch_size, 1, 16, 16)
            
            # ì ì ˆí•œ ì˜¤ë¥˜ ì²˜ë¦¬ë¡œ UNet ì˜ˆì¸¡
            if self.use_conditioning:
                try:
                    noise_pred = self.unet(
                        sample=path_img,
                        timestep=timesteps,
                        encoder_hidden_states=condition_emb.unsqueeze(1),
                        return_dict=False
                    )[0]
                except Exception:
                    try:
                        noise_pred = self.unet(
                            path_img,
                            timesteps,
                            condition_emb.unsqueeze(1),
                            return_dict=False
                        )[0]
                    except Exception:
                        noise_pred = self.unet(path_img, timesteps, return_dict=False)[0]
            else:
                noise_pred = self.unet(path_img, timesteps, return_dict=False)[0]
            
            # ë‹¤ì‹œ ë³€í™˜
            noise_pred_flat = noise_pred.view(batch_size, -1)
            noise_pred_path = self.path_unprojection(noise_pred_flat)
            noise_pred_path = noise_pred_path.view(batch_size, self.max_path_length, self.path_dim)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ë‹¨ê³„
            path = self.scheduler.step(noise_pred_path, timestep, path, return_dict=False)[0]
        
        # ê²½ë¡œê°€ ì‹œì‘ ìœ„ì¹˜ì—ì„œ ì‹œì‘í•˜ê³  ëª©í‘œ ìœ„ì¹˜ì—ì„œ ëë‚˜ë„ë¡ ë³´ì¥
        # path[:, 0] = start_pos
        # path[:, -1] = goal_pos
        
        # ì¤‘ê°„ ì ì„ ë³´ê°„í•˜ì—¬ ê²½ë¡œ ë¶€ë“œëŸ½ê²Œ í•˜ê¸°
        for i in range(1, self.max_path_length - 1):
            alpha = i / (self.max_path_length - 1)
            linear_interp = start_pos * (1 - alpha) + goal_pos * alpha
            path[:, i] = 0.9 * path[:, i] + 0.1 * linear_interp
        
        return path

# ============================================================================
# 5. Dataset Generation and Management
# ============================================================================

def generate_diffusion_dataset(num_samples=2000, image_size=64, max_path_length=16):
    """í™•ì‚° ê²½ë¡œ ê³„íšì„ ìœ„í•œ ë°ì´í„°ì…‹ ìƒì„±"""
    dataset = []
    
    print(f"í™•ì‚° ê²½ë¡œ ê³„íš ìƒ˜í”Œ {num_samples}ê°œ ìƒì„± ì¤‘...")
    
    successful_samples = 0
    attempts = 0
    max_attempts = num_samples * 3
    
    while successful_samples < num_samples and attempts < max_attempts:
        attempts += 1
        
        if attempts % 500 == 0:
            print(f"ì‹œë„: {attempts}, ì„±ê³µ: {successful_samples}/{num_samples}")
        
        # ë„ì „ì ì¸ ì¥ì• ë¬¼ íŒ¨í„´ ìƒì„±
        try:
            rgb_image, obstacle_map, pattern_type = generate_challenging_navigation_scene()
        except:
            continue
        
        # ììœ  ìœ„ì¹˜ ì°¾ê¸°
        free_positions = np.argwhere(obstacle_map == 0)
        
        if len(free_positions) < 20:
            continue
        
        # ì¶©ë¶„í•œ ê±°ë¦¬ë¡œ ì‹œì‘ê³¼ ëª©í‘œ ì„ íƒ
        max_pos_attempts = 20
        valid_pair = False
        
        for _ in range(max_pos_attempts):
            indices = np.random.choice(len(free_positions), 2, replace=False)
            start_pos = free_positions[indices[0]]
            goal_pos = free_positions[indices[1]]
            
            distance = np.linalg.norm(goal_pos - start_pos)
            if distance > image_size * 0.3:
                valid_pair = True
                break
        
        if not valid_pair:
            continue
        
        # A*ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì  ê²½ë¡œ ìƒì„±
        try:
            pathfinder = AStarPathfinder(obstacle_map)
            optimal_path = pathfinder.find_path(start_pos, goal_pos)
        except:
            continue
        
        if len(optimal_path) < 3:
            continue
        
        # ê³ ì • ê¸¸ì´ë¡œ ê²½ë¡œ í¬ì¸íŠ¸ ìƒ˜í”Œë§
        if len(optimal_path) > max_path_length:
            indices = np.linspace(0, len(optimal_path) - 1, max_path_length, dtype=int)
            sampled_path = [optimal_path[i] for i in indices]
        else:
            sampled_path = []
            for i in range(max_path_length):
                t = i / (max_path_length - 1)
                idx = min(int(t * (len(optimal_path) - 1)), len(optimal_path) - 1)
                sampled_path.append(optimal_path[idx])
        
        # numpyë¡œ ë³€í™˜í•˜ê³  ì •ê·œí™”
        path_array = np.array(sampled_path, dtype=np.float32)
        norm_start = start_pos.astype(np.float32) / image_size
        norm_goal = goal_pos.astype(np.float32) / image_size
        norm_path = path_array / image_size
        
        # í…ì„œ ë°ì´í„° ìƒì„±
        obstacle_tensor = torch.FloatTensor(obstacle_map.astype(np.float32))
        rgb_tensor = torch.FloatTensor(rgb_image).permute(2, 0, 1) / 255.0
        path_tensor = torch.FloatTensor(norm_path)
        start_tensor = torch.FloatTensor(norm_start)
        goal_tensor = torch.FloatTensor(norm_goal)
        
        dataset.append({
            'obstacle_map': obstacle_tensor,
            'rgb_image': rgb_tensor,
            'start_pos': start_tensor,
            'goal_pos': goal_tensor,
            'optimal_path': path_tensor,
            'pattern_type': pattern_type
        })
        
        successful_samples += 1
    
    print(f"í™•ì‚° ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(dataset)} ìƒ˜í”Œ")
    return dataset

class DiffusionPathDataset(Dataset):
    def __init__(self, data):
        self.data = data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

# ============================================================================
# 6. Training Loop (FIXED)
# ============================================================================

class SimpleEMA:
    """ê°„ë‹¨í•œ EMA êµ¬í˜„"""
    def __init__(self, model, decay=0.999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def step(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad and name in self.shadow:
                self.shadow[name] = self.decay * self.shadow[name] + (1 - self.decay) * param.data
    
    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad and name in self.shadow:
                param.data = self.shadow[name]

def train_diffusion_model(model, train_loader, val_loader, val_data, num_epochs=100, lr=1e-4, device='cpu', save_dir='./models'):
    """í™•ì‚° ê²½ë¡œ ê³„íš ëª¨ë¸ í›ˆë ¨ (ìˆ˜ì •ë¨)"""
    
    os.makedirs(save_dir, exist_ok=True)
    
    model = model.to(device)
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    ema = SimpleEMA(model)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    
    train_losses = []
    val_losses = []
    
    print(f"ë””ë°”ì´ìŠ¤ì—ì„œ í™•ì‚° ëª¨ë¸ í›ˆë ¨: {device}")
    print(f"ëª¨ë¸ì€ 25 ì—í¬í¬ë§ˆë‹¤ ì €ì¥ë©ë‹ˆë‹¤: {save_dir}")
    
    for epoch in range(num_epochs):
        # í›ˆë ¨ ë‹¨ê³„
        model.train()
        train_loss_sum = 0
        collision_loss_sum = 0
        
        for batch_idx, batch in enumerate(train_loader):
            obstacle_map = batch['obstacle_map'].to(device)
            start_pos = batch['start_pos'].to(device)
            goal_pos = batch['goal_pos'].to(device)
            target_path = batch['optimal_path'].to(device)
            
            batch_size = obstacle_map.shape[0]
            
            # ëœë¤ íƒ€ì„ìŠ¤í… ìƒ˜í”Œë§
            timesteps = torch.randint(
                0, model.scheduler.num_train_timesteps, 
                (batch_size,), device=device
            ).long()
            
            # íƒ€ê²Ÿ ê²½ë¡œì— ë…¸ì´ì¦ˆ ì¶”ê°€
            noise = torch.randn_like(target_path)
            noisy_paths = model.scheduler.add_noise(target_path, noise, timesteps)
            
            # ë…¸ì´ì¦ˆ ì˜ˆì¸¡
            optimizer.zero_grad()
            noise_pred = model(noisy_paths, timesteps, obstacle_map, start_pos, goal_pos)
            
            # ê¸°ë³¸ ë””ë…¸ì´ì§• ì†ì‹¤
            denoising_loss = F.mse_loss(noise_pred, noise)
            
            # ê°•ê±´í•œ ì¶©ëŒ ê°ì§€
            try:
                collision_penalty = compute_noise_based_collision_penalty(
                    noise_pred, noisy_paths, timesteps, model.scheduler, obstacle_map, device
                )
                
                if batch_idx % 20 == 0 and epoch % 5 == 0:
                    print(f"  ì—í¬í¬ {epoch}, ë°°ì¹˜ {batch_idx}")
                    print(f"    ë””ë…¸ì´ì§•: {denoising_loss.item():.6f}")
                    print(f"    ì¶©ëŒ: {collision_penalty.item():.6f}")
                    
            except Exception as e:
                print(f"âŒ ì¶©ëŒ í˜ë„í‹° ì‹¤íŒ¨: {e}")
                collision_penalty = torch.tensor(0.0, device=device, requires_grad=True)
            
            # ê²°í•© ì†ì‹¤
            total_loss = denoising_loss + 1.0 * collision_penalty
            
            if not total_loss.requires_grad:
                print("âš ï¸ ê²½ê³ : ì´ ì†ì‹¤ì´ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ìš”êµ¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
                total_loss = denoising_loss
            
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            ema.step()
            
            train_loss_sum += denoising_loss.item()
            collision_loss_sum += collision_penalty.item()
        
        scheduler.step()
        
        # ê²€ì¦ ë‹¨ê³„
        model.eval()
        val_loss_sum = 0
        
        with torch.no_grad():
            for batch in val_loader:
                obstacle_map = batch['obstacle_map'].to(device)
                start_pos = batch['start_pos'].to(device)
                goal_pos = batch['goal_pos'].to(device)
                target_path = batch['optimal_path'].to(device)
                
                batch_size = obstacle_map.shape[0]
                timesteps = torch.randint(
                    0, model.scheduler.num_train_timesteps,
                    (batch_size,), device=device
                ).long()
                
                noise = torch.randn_like(target_path)
                noisy_paths = model.scheduler.add_noise(target_path, noise, timesteps)
                noise_pred = model(noisy_paths, timesteps, obstacle_map, start_pos, goal_pos)
                
                val_loss = F.mse_loss(noise_pred, noise)
                val_loss_sum += val_loss.item()
        
        avg_train_loss = train_loss_sum / len(train_loader)
        avg_val_loss = val_loss_sum / len(val_loader)
        avg_collision = collision_loss_sum / len(train_loader)
        
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if epoch % 10 == 0:
            print(f"ì—í¬í¬ {epoch:3d}: "
                  f"í›ˆë ¨: {avg_train_loss:.4f}, "
                  f"ê²€ì¦: {avg_val_loss:.4f}, "
                  f"ì¶©ëŒ: {avg_collision:.4f}")
        
        # 25 ì—í¬í¬ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (epoch + 1) % 25 == 0:
            checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch + 1}.pth')
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'collision_loss': avg_collision
            }, checkpoint_path)
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ë¨: {checkpoint_path}")
    
    # EMA ê°€ì¤‘ì¹˜ ì ìš©
    ema.apply_shadow()
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_path = os.path.join(save_dir, 'final_model.pth')
    torch.save(model.state_dict(), final_path)
    print(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ë¨: {final_path}")
    
    return train_losses, val_losses

# ============================================================================
# 7. Evaluation Functions
# ============================================================================

def calculate_path_smoothness(path):
    """ê²½ë¡œ ë¶€ë“œëŸ¬ì›€ ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ ë¶€ë“œëŸ¬ì›€)"""
    if len(path) < 3:
        return 0
    
    curvatures = []
    for i in range(1, len(path) - 1):
        v1 = path[i] - path[i-1]
        v2 = path[i+1] - path[i]
        
        if np.linalg.norm(v1) > 0 and np.linalg.norm(v2) > 0:
            cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
            cos_angle = np.clip(cos_angle, -1, 1)
            angle = np.arccos(cos_angle)
            curvatures.append(angle)
    
    return np.mean(curvatures) if curvatures else 0

def evaluate_path_quality(generated_path, target_path, obstacle_map):
    """ê²½ë¡œ í’ˆì§ˆ í‰ê°€"""
    img_size = 64
    
    # ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
    generated_path_img = generated_path * img_size
    target_path_img = target_path * img_size
    
    metrics = {}
    
    # 1. ì¶©ëŒë¥  ê³„ì‚°
    collisions = 0
    for point in generated_path_img:
        x, y = int(np.clip(point[1], 0, 63)), int(np.clip(point[0], 0, 63))
        if obstacle_map[y, x] > 0.5:
            collisions += 1
    metrics['collision_rate'] = collisions / len(generated_path_img)
    
    # 2. ê²½ë¡œ ê¸¸ì´ ë¹„ìœ¨
    path_length_optimal = np.sum(np.linalg.norm(np.diff(target_path_img, axis=0), axis=1))
    path_length_generated = np.sum(np.linalg.norm(np.diff(generated_path_img, axis=0), axis=1))
    metrics['length_ratio'] = path_length_generated / max(path_length_optimal, 1e-6)
    
    # 3. ë¶€ë“œëŸ¬ì›€ ë¹„ìœ¨
    smoothness_optimal = calculate_path_smoothness(target_path_img)
    smoothness_generated = calculate_path_smoothness(generated_path_img)
    metrics['smoothness_ratio'] = smoothness_generated / max(smoothness_optimal, 1e-6)
    
    # 4. ëª©í‘œ ì˜¤ì°¨
    goal_error = np.linalg.norm(generated_path_img[-1] - target_path_img[-1])
    metrics['goal_error'] = goal_error
    
    return metrics

def load_model_checkpoint(model_path, device='cpu'):
    """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    print(f"ëª¨ë¸ ë¡œë”©: {model_path}")
    
    # ëª¨ë¸ ìƒì„±
    model = PathDiffusionModel(max_path_length=16)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location=device)
        
        # ì²´í¬í¬ì¸íŠ¸ì¸ì§€ ëª¨ë¸ ìƒíƒœë§Œì¸ì§€ í™•ì¸
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            epoch = checkpoint.get('epoch', 'Unknown')
            train_loss = checkpoint.get('train_loss', 'Unknown')
            print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œë¨: ì—í¬í¬ {epoch}, í›ˆë ¨ ì†ì‹¤: {train_loss}")
        else:
            # ëª¨ë¸ ìƒíƒœë§Œ ìˆëŠ” ê²½ìš°
            model.load_state_dict(checkpoint)
            print("ëª¨ë¸ ìƒíƒœ ë¡œë“œë¨")
    else:
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
    
    model.to(device)
    model.eval()
    
    return model

def evaluate_model_on_dataset(model, dataset, device='cpu', num_samples=50):
    """ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ í‰ê°€"""
    model.eval()
    
    all_metrics = {
        'collision_rate': [],
        'length_ratio': [],
        'smoothness_ratio': [],
        'goal_error': []
    }
    
    pattern_metrics = {
        'maze': {'collision_rate': [], 'length_ratio': [], 'smoothness_ratio': [], 'goal_error': []},
        'scattered': {'collision_rate': [], 'length_ratio': [], 'smoothness_ratio': [], 'goal_error': []},
        'corridor': {'collision_rate': [], 'length_ratio': [], 'smoothness_ratio': [], 'goal_error': []}
    }
    
    print(f"ëª¨ë¸ í‰ê°€ ì¤‘... {num_samples} ìƒ˜í”Œ")
    
    with torch.no_grad():
        for i in range(min(num_samples, len(dataset))):
            sample = dataset[i]
            
            try:
                # ê²½ë¡œ ìƒì„±
                obstacle_map = sample['obstacle_map'].unsqueeze(0).to(device)
                start_pos = sample['start_pos'].unsqueeze(0).to(device)
                goal_pos = sample['goal_pos'].unsqueeze(0).to(device)
                target_path = sample['optimal_path'].numpy()
                pattern_type = sample['pattern_type']
                
                generated_path = model.generate_path(obstacle_map, start_pos, goal_pos, num_inference_steps=200)
                generated_path = generated_path.squeeze(0).cpu().numpy()
                
                # í’ˆì§ˆ í‰ê°€
                obstacle_np = obstacle_map.squeeze().cpu().numpy()
                metrics = evaluate_path_quality(generated_path, target_path, obstacle_np)
                
                # ê²°ê³¼ ì €ì¥
                for key, value in metrics.items():
                    all_metrics[key].append(value)
                    pattern_metrics[pattern_type][key].append(value)
                
                if (i + 1) % 10 == 0:
                    print(f"  ì²˜ë¦¬ë¨: {i + 1}/{num_samples}")
                    
            except Exception as e:
                print(f"ìƒ˜í”Œ {i} í‰ê°€ ì‹¤íŒ¨: {e}")
                continue
    
    # í‰ê·  ê³„ì‚°
    avg_metrics = {}
    for key, values in all_metrics.items():
        if values:
            avg_metrics[key] = np.mean(values)
            avg_metrics[f'{key}_std'] = np.std(values)
        else:
            avg_metrics[key] = float('inf')
            avg_metrics[f'{key}_std'] = 0
    
    # íŒ¨í„´ë³„ í‰ê·  ê³„ì‚°
    pattern_avg_metrics = {}
    for pattern, metrics in pattern_metrics.items():
        pattern_avg_metrics[pattern] = {}
        for key, values in metrics.items():
            if values:
                pattern_avg_metrics[pattern][key] = np.mean(values)
                pattern_avg_metrics[pattern][f'{key}_std'] = np.std(values)
            else:
                pattern_avg_metrics[pattern][key] = float('inf')
                pattern_avg_metrics[pattern][f'{key}_std'] = 0
    
    return avg_metrics, pattern_avg_metrics, all_metrics

def visualize_evaluation_results(model, dataset, device='cpu', num_examples=6, save_path=None):
    """í‰ê°€ ê²°ê³¼ ì‹œê°í™”"""
    model.eval()
    
    # ê·¸ë¦¬ë“œ ì„¤ì •
    cols = 3
    rows = num_examples // cols + (num_examples % cols > 0)
    fig, axes = plt.subplots(rows * 2, cols, figsize=(cols * 6, rows * 8))
    
    if rows == 1:
        axes = axes.reshape(2, cols)
    
    examples_shown = 0
    
    with torch.no_grad():
        for i in range(min(num_examples, len(dataset))):
            if examples_shown >= num_examples:
                break
                
            sample = dataset[i * (len(dataset) // num_examples)]
            
            try:
                # ê²½ë¡œ ìƒì„±
                obstacle_map = sample['obstacle_map'].unsqueeze(0).to(device)
                start_pos = sample['start_pos'].unsqueeze(0).to(device)
                goal_pos = sample['goal_pos'].unsqueeze(0).to(device)
                target_path = sample['optimal_path']
                pattern_type = sample['pattern_type']
                
                generated_path = model.generate_path(obstacle_map, start_pos, goal_pos, num_inference_steps=200)
                generated_path = generated_path.squeeze(0).cpu().numpy()
                
                # ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
                img_size = 64
                obstacle_np = obstacle_map.squeeze().cpu().numpy()
                start_img = start_pos.squeeze().cpu().numpy() * img_size
                goal_img = goal_pos.squeeze().cpu().numpy() * img_size
                target_path_img = target_path.numpy() * img_size
                generated_path_img = generated_path * img_size
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
                metrics = evaluate_path_quality(generated_path, target_path.numpy(), obstacle_np)
                
                # í”Œë¡¯ ìœ„ì¹˜ ê³„ì‚°
                row_idx = (examples_shown // cols) * 2
                col_idx = examples_shown % cols
                
                # ì²« ë²ˆì§¸ í–‰: ê²½ë¡œ ë¹„êµ
                ax1 = axes[row_idx, col_idx]
                ax1.imshow(1 - obstacle_np, cmap='gray', alpha=0.8)
                ax1.plot(target_path_img[:, 1], target_path_img[:, 0], 'b-', 
                        linewidth=3, alpha=0.7, label='A* ìµœì ')
                ax1.plot(generated_path_img[:, 1], generated_path_img[:, 0], 'r--', 
                        linewidth=3, alpha=0.7, label='í™•ì‚° ìƒì„±')
                ax1.plot(start_img[1], start_img[0], 'go', markersize=10, label='ì‹œì‘')
                ax1.plot(goal_img[1], goal_img[0], 'ro', markersize=10, label='ëª©í‘œ')
                ax1.set_title(f'{pattern_type}\nì¶©ëŒë¥ : {metrics["collision_rate"]:.3f}')
                ax1.legend(fontsize=8)
                ax1.grid(True, alpha=0.3)
                ax1.set_xlim(0, 64)
                ax1.set_ylim(0, 64)
                
                # ë‘ ë²ˆì§¸ í–‰: ë©”íŠ¸ë¦­ ì •ë³´
                ax2 = axes[row_idx + 1, col_idx]
                metric_text = f"""
í’ˆì§ˆ ë©”íŠ¸ë¦­:

ì¶©ëŒë¥ : {metrics['collision_rate']:.3f}
ê¸¸ì´ ë¹„ìœ¨: {metrics['length_ratio']:.2f}
ë¶€ë“œëŸ¬ì›€ ë¹„ìœ¨: {metrics['smoothness_ratio']:.2f}
ëª©í‘œ ì˜¤ì°¨: {metrics['goal_error']:.2f}

íŒ¨í„´: {pattern_type}
"""
                ax2.text(0.1, 0.5, metric_text, fontsize=10, verticalalignment='center',
                        bbox=dict(boxstyle="round,pad=0.3", 
                                facecolor="lightcyan" if metrics['collision_rate'] < 0.1 else "lightcoral"))
                ax2.set_xlim(0, 1)
                ax2.set_ylim(0, 1)
                ax2.axis('off')
                
                examples_shown += 1
                
            except Exception as e:
                print(f"ì‹œê°í™” ì‹¤íŒ¨ (ìƒ˜í”Œ {i}): {e}")
                continue
    
    # ë¹ˆ ì¶• ìˆ¨ê¸°ê¸°
    for i in range(examples_shown, rows * cols):
        row_idx = (i // cols) * 2
        col_idx = i % cols
        axes[row_idx, col_idx].axis('off')
        axes[row_idx + 1, col_idx].axis('off')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"ì‹œê°í™” ì €ì¥ë¨: {save_path}")
    
    plt.show()

def print_evaluation_report(avg_metrics, pattern_metrics):
    """í‰ê°€ ë³´ê³ ì„œ ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸ“Š ëª¨ë¸ í‰ê°€ ë³´ê³ ì„œ")
    print("="*60)
    
    # ì „ì²´ ë©”íŠ¸ë¦­
    print("\nğŸ¯ ì „ì²´ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
    print(f"  ì¶©ëŒë¥ :     {avg_metrics['collision_rate']:.3f} Â± {avg_metrics['collision_rate_std']:.3f}")
    print(f"  ê¸¸ì´ ë¹„ìœ¨:  {avg_metrics['length_ratio']:.2f} Â± {avg_metrics['length_ratio_std']:.2f}")
    print(f"  ë¶€ë“œëŸ¬ì›€:   {avg_metrics['smoothness_ratio']:.2f} Â± {avg_metrics['smoothness_ratio_std']:.2f}")
    print(f"  ëª©í‘œ ì˜¤ì°¨:  {avg_metrics['goal_error']:.2f} Â± {avg_metrics['goal_error_std']:.2f}")
    
    # ì„±ëŠ¥ ë“±ê¸‰
    collision_grade = "ìš°ìˆ˜" if avg_metrics['collision_rate'] < 0.05 else "ì–‘í˜¸" if avg_metrics['collision_rate'] < 0.15 else "ê°œì„  í•„ìš”"
    length_grade = "ìš°ìˆ˜" if 0.9 <= avg_metrics['length_ratio'] <= 1.3 else "ì–‘í˜¸" if 0.8 <= avg_metrics['length_ratio'] <= 1.5 else "ê°œì„  í•„ìš”"
    
    print(f"\nğŸ“ˆ ì„±ëŠ¥ ë“±ê¸‰:")
    print(f"  ì¶©ëŒ íšŒí”¼:  {collision_grade}")
    print(f"  ê²½ë¡œ íš¨ìœ¨:  {length_grade}")
    
    # íŒ¨í„´ë³„ ì„±ëŠ¥
    print(f"\nğŸ—ï¸ íŒ¨í„´ë³„ ì„±ëŠ¥:")
    for pattern, metrics in pattern_metrics.items():
        print(f"\n  {pattern.upper()} íŒ¨í„´:")
        print(f"    ì¶©ëŒë¥ :   {metrics['collision_rate']:.3f}")
        print(f"    ê¸¸ì´ ë¹„ìœ¨: {metrics['length_ratio']:.2f}")
        print(f"    ë¶€ë“œëŸ¬ì›€:  {metrics['smoothness_ratio']:.2f}")
        print(f"    ëª©í‘œ ì˜¤ì°¨: {metrics['goal_error']:.2f}")
    
    print("\n" + "="*60)

# ============================================================================
# 8. Main Functions
# ============================================================================

def main_training():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    print("í™•ì‚° ê²½ë¡œ ê³„íš ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    dataset = generate_diffusion_dataset(num_samples=1500, max_path_length=16)
    
    if len(dataset) < 100:
        print(f"âŒ {len(dataset)}ê°œ ìƒ˜í”Œë§Œ ìƒì„±ë¨. ë” ë§ì€ ìƒ˜í”Œì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return None
    
    # ë°ì´í„°ì…‹ ë¶„í• 
    train_size = int(0.8 * len(dataset))
    train_data = dataset[:train_size]
    val_data = dataset[train_size:]
    
    print(f"ë°ì´í„°ì…‹ ë¶„í• : í›ˆë ¨={len(train_data)}, ê²€ì¦={len(val_data)}")
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_dataset = DiffusionPathDataset(train_data)
    val_dataset = DiffusionPathDataset(val_data)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
    
    # ëª¨ë¸ ìƒì„±
    model = PathDiffusionModel(max_path_length=16)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"í™•ì‚° ëª¨ë¸ íŒŒë¼ë¯¸í„°: {total_params:,}")
    
    # ëª¨ë¸ í›ˆë ¨
    print("\ní™•ì‚° ê²½ë¡œ ê³„íš í›ˆë ¨ ì‹œì‘...")
    train_losses, val_losses = train_diffusion_model(
        model, train_loader, val_loader, val_data,
        num_epochs=100, lr=1e-4, device=device
    )
    
    print("\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
    return model

def main_evaluation(model_path, num_samples=100, num_visualize=6):
    """ë©”ì¸ í‰ê°€ í•¨ìˆ˜"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        model = load_model_checkpoint(model_path, device)
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # í‰ê°€ìš© ë°ì´í„°ì…‹ ìƒì„±
    print("\ní‰ê°€ìš© ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    eval_dataset = generate_diffusion_dataset(num_samples=200, max_path_length=16)
    
    if len(eval_dataset) < 50:
        print(f"âŒ í‰ê°€ìš© ë°ì´í„°ì…‹ì´ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(eval_dataset)} ìƒ˜í”Œ")
        return
    
    print(f"í‰ê°€ìš© ë°ì´í„°ì…‹: {len(eval_dataset)} ìƒ˜í”Œ")
    
    # ëª¨ë¸ í‰ê°€
    print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    avg_metrics, pattern_metrics, all_metrics = evaluate_model_on_dataset(
        model, eval_dataset, device, num_samples=num_samples
    )
    
    # í‰ê°€ ë³´ê³ ì„œ ì¶œë ¥
    print_evaluation_report(avg_metrics, pattern_metrics)
    
    # ì‹œê°í™”
    print(f"\nğŸ¨ {num_visualize}ê°œ ìƒ˜í”Œ ì‹œê°í™” ì¤‘...")
    timestamp = str(int(os.path.getctime(model_path)))
    save_path = f"./evaluation_results_{timestamp}.png"
    
    visualize_evaluation_results(
        model, eval_dataset, device, 
        num_examples=num_visualize, save_path=save_path
    )
    
    return avg_metrics, pattern_metrics

def main():
    """ë©”ì¸ í•¨ìˆ˜ - ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬"""
    parser = argparse.ArgumentParser(description='í™•ì‚° ê¸°ë°˜ ê²½ë¡œ ê³„íš - í›ˆë ¨ ë° í‰ê°€')
    parser.add_argument('--mode', choices=['train', 'eval'], required=True,
                       help='ì‹¤í–‰ ëª¨ë“œ: train (í›ˆë ¨) ë˜ëŠ” eval (í‰ê°€)')
    parser.add_argument('--model_path', type=str, default='./models/final_model.pth',
                       help='í‰ê°€í•  ëª¨ë¸ ê²½ë¡œ (í‰ê°€ ëª¨ë“œìš©)')
    parser.add_argument('--num_samples', type=int, default=100,
                       help='í‰ê°€í•  ìƒ˜í”Œ ìˆ˜')
    parser.add_argument('--num_visualize', type=int, default=6,
                       help='ì‹œê°í™”í•  ì˜ˆì œ ìˆ˜')
    
    args = parser.parse_args()
    
    if args.mode == 'train':
        print("ğŸš€ í›ˆë ¨ ëª¨ë“œ ì‹œì‘")
        main_training()
        
    elif args.mode == 'eval':
        print("ğŸ“Š í‰ê°€ ëª¨ë“œ ì‹œì‘")
        if not os.path.exists(args.model_path):
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {args.model_path}")
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ì°¾ê¸°
            models_dir = Path('./models')
            if models_dir.exists():
                model_files = list(models_dir.glob('*.pth'))
                if model_files:
                    print("\nì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:")
                    for i, model_file in enumerate(model_files):
                        print(f"  {i+1}. {model_file}")
                    
                    choice = input("\nì‚¬ìš©í•  ëª¨ë¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 1): ").strip()
                    try:
                        choice_idx = int(choice) - 1 if choice else 0
                        args.model_path = str(model_files[choice_idx])
                        print(f"ì„ íƒëœ ëª¨ë¸: {args.model_path}")
                    except (ValueError, IndexError):
                        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ì²« ë²ˆì§¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        args.model_path = str(model_files[0])
                else:
                    print("âŒ ./models í´ë”ì— ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    return
            else:
                print("âŒ ./models í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return
        
        main_evaluation(args.model_path, args.num_samples, args.num_visualize)

def interactive_evaluation():
    """ëŒ€í™”í˜• í‰ê°€ ëª¨ë“œ"""
    print("ğŸ® ëŒ€í™”í˜• í‰ê°€ ëª¨ë“œ")
    print("="*50)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì°¾ê¸°
    models_dir = Path('./models')
    if not models_dir.exists():
        print("âŒ ./models í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    model_files = list(models_dir.glob('*.pth'))
    if not model_files:
        print("âŒ ./models í´ë”ì— ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("\nğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:")
    for i, model_file in enumerate(model_files):
        # íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        stat = model_file.stat()
        size_mb = stat.st_size / (1024 * 1024)
        mtime = os.path.getctime(str(model_file))
        
        print(f"  {i+1}. {model_file.name}")
        print(f"      í¬ê¸°: {size_mb:.1f}MB")
        print(f"      ìƒì„±: {np.datetime64(int(mtime), 's')}")
        print()
    
    # ëª¨ë¸ ì„ íƒ
    while True:
        try:
            choice = input("í‰ê°€í•  ëª¨ë¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (1-{}): ".format(len(model_files))).strip()
            choice_idx = int(choice) - 1
            if 0 <= choice_idx < len(model_files):
                selected_model = str(model_files[choice_idx])
                break
            else:
                print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
        except ValueError:
            print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    print(f"\nâœ… ì„ íƒëœ ëª¨ë¸: {Path(selected_model).name}")
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        model = load_model_checkpoint(selected_model, device)
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    while True:
        print("\n" + "="*50)
        print("ğŸ¯ í‰ê°€ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ë¹ ë¥¸ í‰ê°€ (50 ìƒ˜í”Œ)")
        print("2. í‘œì¤€ í‰ê°€ (100 ìƒ˜í”Œ)")
        print("3. ì •ë°€ í‰ê°€ (200 ìƒ˜í”Œ)")
        print("4. ì‹œê°í™”ë§Œ (6 ì˜ˆì œ)")
        print("5. ì»¤ìŠ¤í…€ ì„¤ì •")
        print("0. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ (0-5): ").strip()
        
        if choice == '0':
            break
        elif choice == '1':
            num_samples, num_visualize = 50, 6
        elif choice == '2':
            num_samples, num_visualize = 100, 6
        elif choice == '3':
            num_samples, num_visualize = 200, 9
        elif choice == '4':
            num_samples, num_visualize = 10, 6
        elif choice == '5':
            try:
                num_samples = int(input("í‰ê°€ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ 100): ") or "100")
                num_visualize = int(input("ì‹œê°í™” ì˜ˆì œ ìˆ˜ (ê¸°ë³¸ 6): ") or "6")
                num_samples = max(10, min(500, num_samples))
                num_visualize = max(3, min(12, num_visualize))
            except ValueError:
                print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                num_samples, num_visualize = 100, 6
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
            continue
        
        print(f"\nğŸš€ í‰ê°€ ì‹œì‘: {num_samples} ìƒ˜í”Œ, {num_visualize} ì‹œê°í™”")
        
        # í‰ê°€ìš© ë°ì´í„°ì…‹ ìƒì„±
        eval_dataset = generate_diffusion_dataset(num_samples=num_samples + 50, max_path_length=16)
        
        if len(eval_dataset) < 20:
            print(f"âŒ í‰ê°€ìš© ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨")
            continue
        
        # í‰ê°€ ì‹¤í–‰
        try:
            avg_metrics, pattern_metrics, all_metrics = evaluate_model_on_dataset(
                model, eval_dataset, device, num_samples=num_samples
            )
            
            # ê²°ê³¼ ì¶œë ¥
            print_evaluation_report(avg_metrics, pattern_metrics)
            
            # ì‹œê°í™” ì—¬ë¶€ í™•ì¸
            vis_choice = input("\nì‹œê°í™”ë¥¼ í‘œì‹œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if vis_choice in ['y', 'yes']:
                timestamp = str(int(time.time()))
                save_path = f"./evaluation_{timestamp}.png"
                
                visualize_evaluation_results(
                    model, eval_dataset, device, 
                    num_examples=num_visualize, save_path=save_path
                )
            
            # ê²°ê³¼ ì €ì¥ ì—¬ë¶€ í™•ì¸
            save_choice = input("\nê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if save_choice in ['y', 'yes']:
                save_evaluation_results(avg_metrics, pattern_metrics, all_metrics, selected_model)
                
        except Exception as e:
            print(f"âŒ í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            continue

def save_evaluation_results(avg_metrics, pattern_metrics, all_metrics, model_path):
    """í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    import json
    import time
    
    timestamp = int(time.time())
    results = {
        'timestamp': timestamp,
        'model_path': model_path,
        'average_metrics': avg_metrics,
        'pattern_metrics': pattern_metrics,
        'detailed_metrics': {key: [float(v) for v in values] for key, values in all_metrics.items()}
    }
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    results_file = f"./evaluation_results_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… í‰ê°€ ê²°ê³¼ ì €ì¥ë¨: {results_file}")
    
    # í…ìŠ¤íŠ¸ ë³´ê³ ì„œë„ ìƒì„±
    report_file = f"./evaluation_report_{timestamp}.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("="*60 + "\n")
        f.write("ğŸ“Š ëª¨ë¸ í‰ê°€ ë³´ê³ ì„œ\n")
        f.write("="*60 + "\n")
        f.write(f"ëª¨ë¸ ê²½ë¡œ: {model_path}\n")
        f.write(f"í‰ê°€ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n\n")
        
        f.write("ğŸ¯ ì „ì²´ ì„±ëŠ¥ ë©”íŠ¸ë¦­:\n")
        f.write(f"  ì¶©ëŒë¥ :     {avg_metrics['collision_rate']:.3f} Â± {avg_metrics['collision_rate_std']:.3f}\n")
        f.write(f"  ê¸¸ì´ ë¹„ìœ¨:  {avg_metrics['length_ratio']:.2f} Â± {avg_metrics['length_ratio_std']:.2f}\n")
        f.write(f"  ë¶€ë“œëŸ¬ì›€:   {avg_metrics['smoothness_ratio']:.2f} Â± {avg_metrics['smoothness_ratio_std']:.2f}\n")
        f.write(f"  ëª©í‘œ ì˜¤ì°¨:  {avg_metrics['goal_error']:.2f} Â± {avg_metrics['goal_error_std']:.2f}\n\n")
        
        f.write("ğŸ—ï¸ íŒ¨í„´ë³„ ì„±ëŠ¥:\n")
        for pattern, metrics in pattern_metrics.items():
            f.write(f"\n  {pattern.upper()} íŒ¨í„´:\n")
            f.write(f"    ì¶©ëŒë¥ :   {metrics['collision_rate']:.3f}\n")
            f.write(f"    ê¸¸ì´ ë¹„ìœ¨: {metrics['length_ratio']:.2f}\n")
            f.write(f"    ë¶€ë“œëŸ¬ì›€:  {metrics['smoothness_ratio']:.2f}\n")
            f.write(f"    ëª©í‘œ ì˜¤ì°¨: {metrics['goal_error']:.2f}\n")
    
    print(f"âœ… í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥ë¨: {report_file}")

# ì‚¬ìš©ì„± ê°œì„ ì„ ìœ„í•œ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

def find_best_model(models_dir='./models'):
    """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìë™ ì°¾ê¸°"""
    models_path = Path(models_dir)
    if not models_path.exists():
        return None
    
    model_files = list(models_path.glob('*.pth'))
    if not model_files:
        return None
    
    # final_model.pthê°€ ìˆìœ¼ë©´ ìš°ì„  ì„ íƒ
    final_model = models_path / 'final_model.pth'
    if final_model.exists():
        return str(final_model)
    
    # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê°€ì¥ ë†’ì€ ì—í¬í¬ì˜ ì²´í¬í¬ì¸íŠ¸ ì„ íƒ
    checkpoint_files = [f for f in model_files if 'checkpoint_epoch_' in f.name]
    if checkpoint_files:
        # ì—í¬í¬ ë²ˆí˜¸ë¡œ ì •ë ¬
        checkpoint_files.sort(key=lambda x: int(x.name.split('_')[-1].split('.')[0]))
        return str(checkpoint_files[-1])  # ê°€ì¥ ë†’ì€ ì—í¬í¬
    
    # ê·¸ê²ƒë„ ì—†ìœ¼ë©´ ê°€ì¥ ìµœê·¼ íŒŒì¼
    model_files.sort(key=lambda x: x.stat().st_mtime)
    return str(model_files[-1])

def quick_demo(model_path=None):
    """ë¹ ë¥¸ ë°ëª¨ ì‹¤í–‰"""
    print("ğŸ® ë¹ ë¥¸ ë°ëª¨ ëª¨ë“œ")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ëª¨ë¸ ìë™ ì°¾ê¸° ë˜ëŠ” ì§€ì •ëœ ê²½ë¡œ ì‚¬ìš©
    if model_path is None:
        model_path = find_best_model()
        if model_path is None:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        print(f"ğŸ“ ìë™ ì„ íƒëœ ëª¨ë¸: {Path(model_path).name}")
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        model = load_model_checkpoint(model_path, device)
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ê°„ë‹¨í•œ í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±
    print("\nğŸ“Š ê°„ë‹¨í•œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    eval_dataset = generate_diffusion_dataset(num_samples=50, max_path_length=16)
    
    # ë¹ ë¥¸ í‰ê°€
    avg_metrics, _, _ = evaluate_model_on_dataset(model, eval_dataset, device, num_samples=30)
    
    # ê°„ë‹¨í•œ ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ¯ ì„±ëŠ¥ ìš”ì•½:")
    print(f"  ì¶©ëŒë¥ : {avg_metrics['collision_rate']:.3f} ({'ìš°ìˆ˜' if avg_metrics['collision_rate'] < 0.05 else 'ì–‘í˜¸' if avg_metrics['collision_rate'] < 0.15 else 'ê°œì„ í•„ìš”'})")
    print(f"  ê²½ë¡œ íš¨ìœ¨: {avg_metrics['length_ratio']:.2f} ({'ìš°ìˆ˜' if 0.9 <= avg_metrics['length_ratio'] <= 1.3 else 'ì–‘í˜¸' if 0.8 <= avg_metrics['length_ratio'] <= 1.5 else 'ê°œì„ í•„ìš”'})")
    
    # ì‹œê°í™”
    print("\nğŸ¨ ì‹œê°í™” ìƒì„± ì¤‘...")
    visualize_evaluation_results(model, eval_dataset, device, num_examples=3)

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ì—…ë°ì´íŠ¸
if __name__ == "__main__":
    import sys
    import time
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ê°€ ì—†ìœ¼ë©´ ëŒ€í™”í˜• ëª¨ë“œ
    if len(sys.argv) == 1:
        print("ğŸ® ëŒ€í™”í˜• ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        print("ëª…ë ¹í–‰ ì˜µì…˜ì„ ì›í•˜ì‹œë©´: python script.py --help")
        
        print("\nì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("1. í›ˆë ¨ (Training)")
        print("2. ëŒ€í™”í˜• í‰ê°€ (Interactive Evaluation)")  
        print("3. ë¹ ë¥¸ ë°ëª¨ (Quick Demo)")
        print("0. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ (0-3): ").strip()
        
        if choice == '1':
            main_training()
        elif choice == '2':
            interactive_evaluation()
        elif choice == '3':
            quick_demo()
        elif choice == '0':
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
        else:
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
    else:
        # ëª…ë ¹í–‰ ì¸ìˆ˜ê°€ ìˆìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        main()